{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przykładowe problemy związane ze skalowalnością zadań ML:\n",
    "\n",
    "Ograniczenie CPU: Dane mieszczą się w pamięci RAM, ale proces uczenia trwa za długo. Np. W przypadku konieczności sprawdzenia wielu kombinacji parametrów modelu, wielu modeli, itd. \n",
    "\n",
    "\n",
    "Ograniczenia pamięci: Dane są na tyle duże, że nie mieszczą się w pamięci RAM.\n",
    "\n",
    "\n",
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/ml-Pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../img/ml-PipelineModel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potok przetwarzania ML\n",
    "\n",
    "* <b>DataFrame</b>: interfejs API ML używa DataFrame, w którym można przechowywać różne typy danych. Na przykład DataFrame może mieć różne kolumny przechowujące tekst, wektory cech, prawdziwe etykiety i prognozy.\n",
    "\n",
    "* <b>Transformer</b>: Transformator to algorytm, który może przekształcić jedną ramkę danych w inną ramkę danych. Na przykład model ML to transformator, który przekształca ramkę danych z funkcjami w ramkę danych z prognozami. Innym przykładem transformatora jest StringIndexer, który koduje zmienne tekstowe jako zmienne całkowito liczbowe.\n",
    "\n",
    "\n",
    "* <b>Estimator</b>: Estymator to algorytm, który który można zaaplikować do DataFrame w celu wytworzenia transformatora. Np. Algorytm uczenia się jest estymatorem, który trenuje się na DataFrame i tworzy model.\n",
    "\n",
    "\n",
    "* <b>Pipeline</b>: Potok przetwarzania łączy wiele transformatorów i estymatorów razem, aby określić przepływ pracy ML.\n",
    "\n",
    "\n",
    "* <b>Parametr</b>: Wszystkie transformatory i estymatory mają teraz wspólny interfejs API do określania parametrów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_name = 'yourGitHubUsername' # TODO change into your GitHub username"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession \\\n",
    ".builder \\\n",
    ".config('spark.driver.memory','1g') \\\n",
    ".config('spark.executor.memory', '2g') \\\n",
    ".getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semester = '2024l' # TODO set appropriate semester\n",
    "user_id = 9903 # TODO set appropriate user id\n",
    "\n",
    "gs_path = f'gs://ds-{semester}-{user_id}-notebook-data/survey/2020/survey_results_public.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = user_name.replace('-','_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"survey_2020\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'DROP TABLE IF EXISTS {table_name}')\n",
    "\n",
    "spark.sql(f'CREATE TABLE IF NOT EXISTS {table_name} \\\n",
    "          USING csv \\\n",
    "          OPTIONS (HEADER true, INFERSCHEMA true, NULLVALUE \"NA\") \\\n",
    "          LOCATION \"{gs_path}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f'describe {table_name}').show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przygotowanie danych do analizy\n",
    "\n",
    "W ramach zadania chcemy stworzyć klasyfikator, który będzie przewidywać czy respondent zarabia więcej niż 60000 USD rocznie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df= spark.sql(f'SELECT *, CAST((convertedComp > 60000) AS STRING) AS compAboveAvg \\\n",
    "                    FROM {table_name} where convertedComp IS NOT NULL ')\n",
    "spark_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<B>Dążymy do tego, żeby przygotować jeden wektor cech oraz jedną kolumnę z oznaczeniami.</B>\n",
    "\n",
    "Kodujemy kolumny tekstowe na numeryczne oraz kodujemy wartości liczbowe na reprezentacje onehotencoder. Następnie dokonujemy asemblacji do jednego wektora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "# chcemy przewidziec compAboveAvg\n",
    "y = 'compAboveAvg'\n",
    "# na podstawie:\n",
    "feature_columns = ['OpSys', 'EdLevel', 'MainBranch' , 'Country', 'JobSeek', 'YearsCode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zaczynamy od transformatora StringIndexer, zamieniajacego wartosci 'string' na liczbe\n",
    "# dla cech, ktore zostana wykorzystane do predykcji\n",
    "\n",
    "##### najpierw pokazujemy prosta petle z FOR, a potem zrefactorujmy do list comprehension\n",
    "stringindexer_stages_1 = []\n",
    "for c in feature_columns:\n",
    "    stringindexer_stages_1.append (StringIndexer(inputCol=c, outputCol='stringindexed_' + c).setHandleInvalid(\"keep\"))\n",
    "\n",
    "# i dla zmiennej objaśnianej\n",
    "stringindexer_stages_1.append(StringIndexer(inputCol=y, outputCol='label'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>handleInvalid</b> = How to handle invalid data during transform(). Options are 'keep' (invalid data presented as an extra categorical feature) or error (throw an error). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactoring do list comprehension\n",
    "stringindexer_stages = [StringIndexer(inputCol=c, outputCol='stringindexed_' + c).setHandleInvalid(\"keep\") for c in feature_columns]\n",
    "\n",
    "# i dla zmiennej objaśnianej\n",
    "stringindexer_stages += [StringIndexer(inputCol=y, outputCol='label')]\n",
    "stringindexer_stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po wykonaniu takiej transformacji do DF zostaje dodane  7 nowych kolumn z prefixem \"stringindexed_\"\n",
    "Pipeline(stages=stringindexer_stages).fit(spark_df).transform(spark_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehotencoder_stages = [OneHotEncoder(inputCol='stringindexed_' + c, outputCol='onehot_' + c) for c in feature_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozbudowujemy pipeline:\n",
    "\n",
    "Po wykonaniu takiej transformacji (stringIndexer+onehotencoder) do DF zostaje dodane  6 nowych kolumn z prefixem \"onehot_\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pa = Pipeline(stages=stringindexer_stages + onehotencoder_stages).fit(spark_df).transform(spark_df).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowe kolumny zawieraja wartosci typu SparseVector zawierajacy mape bitowa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"https://miro.medium.com/max/2400/1*ggtP4a5YaRx6l09KQaYOnw.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Orginal values:\")\n",
    "print(pa['OpSys'].unique())\n",
    "print (\"---------\")\n",
    "print(\"StringIndexed values:\")\n",
    "print(pa['stringindexed_OpSys'].unique())\n",
    "print (\"---------\")\n",
    "print(\"OneHot values:\")\n",
    "print(pa['onehot_OpSys'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <B>Asemblacja</B> - połączenie wszystkich kolumn predykcyjnych do jednej (kolumna features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_columns = ['onehot_' + c for c in feature_columns]\n",
    "vectorassembler_stage = VectorAssembler(inputCols=extracted_columns, outputCol='features') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Połączenie wszystkich krokþw przygotowania danych w jednym potoku przetwarzania (pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wybór kolumn do ostatecznej ramki danych\n",
    "# poza kolumnami features i label (które będą wykorzystywane do budowania modelu) \n",
    "# zostawiamy m.in. oryginalne kolumn (feature_columns) \n",
    "final_columns = [y] + feature_columns + extracted_columns + ['features', 'label']\n",
    "final_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = Pipeline(stages=stringindexer_stages + \\\n",
    "                          onehotencoder_stages + \\\n",
    "                          [vectorassembler_stage]).fit(spark_df).transform(spark_df).select(final_columns)\n",
    "\n",
    "transformed_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podzial na zbior treningowy/testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, test = transformed_df.randomSplit([0.8, 0.2], seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uczenie modelu - model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na poczatek wybierzemy drzewo decyzyjne. Nie musimy podawac zadnych parametrow\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol='features', labelCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model = Pipeline(stages=[dt]).fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_model.stages[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predykcja - model.transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_simple = simple_model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
    "pred_simple.limit(5).select(show_columns).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ewaluacje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# macierz pomyłek (confusion matrix)\n",
    "label_and_pred = pred_simple.select('label', 'prediction')\n",
    "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ewaluator \n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_simple = evaluator.evaluate(pred_simple)\n",
    "auroc_simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_m = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator_m.evaluate(pred_simple)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dodanie hiperparametrów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jakie wartości hiperparametru maxDepth mają być przetestowane:\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "param_grid = ParamGridBuilder().\\\n",
    "    addGrid(dt.maxDepth, [2,3,4,5,6]).\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Walidacja krzyżowa wykonwyana w celu optymalizacji hiperparametrów\n",
    "from pyspark.ml.tuning import CrossValidator\n",
    "cv = CrossValidator(estimator=dt, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Budowa modelu na podstawie danych treningowych\n",
    "cv_model = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predykcja z nowym modelem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jak wygląda predykcja na zbiorze danych testowych?\n",
    "pred_cv = cv_model.transform(test)\n",
    "show_columns = final_columns + ['prediction', 'rawPrediction', 'probability']\n",
    "pred_cv.limit(5).select(show_columns).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "label_and_pred = pred_cv.select('label', 'prediction')\n",
    "label_and_pred.groupBy('label', 'prediction').count().toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auroc_cv = evaluator.evaluate(pred_cv)\n",
    "auroc_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_cv = evaluator_m.evaluate(pred_cv)\n",
    "acc_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Klasyfikacja za pomoca Gradient Boosted Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "model = gbt.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.evaluate(model.transform(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadania:\n",
    "\n",
    "* Czy mozna jeszcze poprawic jakosc predykcji: \n",
    "    * a) dodajac cechy\n",
    "    * b) zmieniajac model\n",
    "    * c) lepiej dobierajac parametry modelu ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Kod w R\n",
    "#library(data.table)\n",
    "#srv <- fread(\"survey_results_public.csv\")\n",
    "#srv$OpSys2 <- srv$OpSys == \"Windows\"\n",
    "#library(rpart)\n",
    "#srv$CompAboveAvg <- CompAboveAvg$ConvertedComp > 60e3\n",
    "#dt_fit = rpart(CompAboveAvg ~ Age + EdLevel + JobSeek + OpSys + YearsCode , data = srv, method = 'class')\n",
    "#pred_y = predict(dt_fit, type = 'class')\n",
    "#table(predict(dt_fit, srv[,c(\"Age\" , \"EdLevel\", \"JobSeek\", \"OpSys\", \"YearsCode\")], type = \"class\"), srv$CompAboveAvg)\n",
    "#srv(cor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "datascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "notebook_test": {
   "keytab_path": "/data/work/home/ds-lab-testuser1/ds-lab-testuser1.keytab",
   "user": "ds-lab-testuser1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
